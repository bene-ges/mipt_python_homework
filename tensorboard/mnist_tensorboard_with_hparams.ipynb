{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ced8b991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tb-nightly\n",
      "  Downloading tb_nightly-2.8.0a20211215-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from tb-nightly) (58.0.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from tb-nightly) (0.37.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Using cached grpcio-1.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from tb-nightly) (1.21.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Using cached protobuf-3.19.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: six in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from absl-py>=0.4->tb-nightly) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from markdown>=2.6.8->tb-nightly) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aleksandraa/miniconda3/envs/dl/lib/python3.9/site-packages (from requests<3,>=2.21.0->tb-nightly) (2021.10.8)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tb-nightly\n",
      "Successfully installed absl-py-1.0.0 cachetools-4.2.4 charset-normalizer-2.0.9 google-auth-2.3.3 google-auth-oauthlib-0.4.6 grpcio-1.42.0 idna-3.3 markdown-3.3.6 oauthlib-3.1.1 protobuf-3.19.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.8 tb-nightly-2.8.0a20211215 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 urllib3-1.26.7 werkzeug-2.0.2\n"
     ]
    }
   ],
   "source": [
    "! pip install tb-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d843ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa1eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 3, num_classes = 10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16*7*7, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb982db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device=\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f1145f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "#learning_rate = 0.001\n",
    "in_channels = 1\n",
    "num_classes  =10\n",
    "#batch_size = 64\n",
    "num_epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b544142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='dataset/', train=True, \n",
    "                              transform=transforms.ToTensor(), download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bffb86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss in this epoch was 2.3554092949787777\n",
      "Mean loss in this epoch was 2.3067014840682347\n",
      "Mean loss in this epoch was 0.13581883423883342\n",
      "Mean loss in this epoch was 0.329952061366259\n",
      "Mean loss in this epoch was 2.3437147082042085\n",
      "Mean loss in this epoch was 0.15658825641321236\n",
      "Mean loss in this epoch was 0.35211244492785637\n",
      "Mean loss in this epoch was 1.1889367956342474\n",
      "Mean loss in this epoch was 0.6846808078935591\n",
      "Mean loss in this epoch was 0.47247593104839325\n",
      "Mean loss in this epoch was 1.4439357802019281\n",
      "Mean loss in this epoch was 2.271521325838768\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [2, 64, 1024]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        model = CNN(in_channels=in_channels, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "        writer = SummaryWriter(f'runs/MNIST/MiniBatchSize {batch_size} LR {learning_rate}')\n",
    "\n",
    "        step = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "\n",
    "            for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "                data = data.to(device=device)\n",
    "                targets = targets.to(device=device)\n",
    "\n",
    "                #forward\n",
    "                scores = model(data)\n",
    "                loss = criterion(scores, targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                #backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calculate 'running' training accuracy\n",
    "                _, predictions = scores.max(1)\n",
    "                num_correct = (predictions == targets).sum()\n",
    "                running_train_acc = float(num_correct) / float(data.shape[0])\n",
    "                accuracies.append(running_train_acc)\n",
    "\n",
    "                writer.add_scalar('Training loss', loss, global_step = step)\n",
    "                writer.add_scalar('Training accuracy', running_train_acc, global_step=step)\n",
    "                step += 1\n",
    "            \n",
    "            writer.add_hparams({'lr': learning_rate, 'bsize': batch_size},\n",
    "                              {'accuracy': sum(accuracies)/len(accuracies),\n",
    "                              'loss': sum(losses)/len(losses)\n",
    "                               })\n",
    "            print(f'Mean loss in this epoch was {sum(losses)/len(losses)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be555c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
